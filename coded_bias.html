<!-- This is an HTML page for the JNL221 class' first repository.-->

<!DOCTYPE html>
<html>
	<head>
	
		<meta charset="utf-8">
		<title>First repository</title>
		<link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,400,300,600,700&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
		<link href='https://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>
	
		<!-- This is where this page's style sheet is defined. -->
		<link type="text/css" rel="stylesheet" href="index.css" />

	</head>
	<body>
	
		<!-- This is the header row for this page. -->
		<div id="intro">
			<h1>Savannah</h1>
			<h4>Syracuse University, Fall 2025</h4>
		</div>

		<section>
			<p>In the documentary, American political scientist Virginia Eubanks says, “the future is already here, it’s just not evenly distributed.” She shares after an anecdote about a lack of safeguards around biometric data gathering by law enforcement in places like the UK and China. With this quote, she provides greater context about the dispersal of technology across both the world and within microcosms of our society. We may assume wealthier people receive the most prime, flashy, up-to-date technology, leaving poorer individuals behind in getting insight on these developments. However, what most surprised me is that this assumption could not be further from the truth. In fact, it’s poorer people who are assigned the most invasive surveillance systems early on. To explain this, they used the story of the residents in the Brownsville apartments who felt as though a new facial recognition technology encroached upon their privacy, leaving them feeling less in control of who could possibly get ahold of their identities.</p>
			<p>I often see AI affecting algorithms on social media apps like Instagram and TikTok. For me, I notice AI produces personalized content by gathering my user data to curate my feed’s content, showing me repeated mentions of anything from breaking news stories and personal interests to new music drops and products to shop. The documentary mentioned how the output of AI-controlled content is defined between rich and poor. I would argue that there is a growing blur between content catering to specific socioeconomic groups. Today, it is much more common to find that when you like one video about a topic, 100 more videos will come up relating to it. This makes it so many people from many backgrounds can view similar content about the same topic.</p>
			<p>Questions: Across each of these incidents, how is harm categorized? How might one go about assessing the severity of an incident? Who is most likely to use the data provided? Hypothesis: Incidents that occur by fault of AI facial recognition systems show a disproportionate connection to discrimination-based harm.</p>
		</section>

		<div id="end">
			<h4>this page was published on github pages. fonts: montserrat, open sans.</h4>
		</div>


	</body>
</html>